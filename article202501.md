# 1/11(Sat)
<details><summary>8つの質問で自分自身の答えを批評する哲学的手法を活用したLLMのプロンプティング技術</summary>

## 背景  
ChatGPTリリース以降、様々な企業や組織から新たなLLMが次々と発表され、性能の向上が進められてきた。

しかし現状のLLMは、重要な課題が残されている。複雑な推論能力を必要とするタスクにおいて、モデルは依然として誤りを起こしやすい傾向にある。研究者たちの分析によれば、LLMは学習データの記憶と再現には長けているものの、未知の問題や学習時に見たことのない推論課題への対応には苦心していることが指摘されている。実際、トレーニングデータに含まれていないパターンの推論問題に直面した際、パフォーマンスが著しく低下することが確認されている。

LLMの性能を向上させるための手法として、「思考の連鎖」と呼ばれるアプローチが開発されてきた。推論の過程を段階的に示すことで、モデルの論理的思考を強化しようという試みである。

一方で人間の思考プロセスに関する研究分野では、Toulminという哲学者によって提唱された「議論モデル」という理論が支持されている。人間の推論において重要なのは最終的な結論ではなく、その結論に至るまでの根拠や思考過程であるという考えが体系化されたものである。さらに、議論の妥当性を検証するための「批判的質問」という手法も確立されている。

そのような背景のもと、研究者らはToulminの議論モデルと批判的質問の概念をLLMに応用し、モデルの推論能力を向上させる新たな手法の開発に取り組んだ。従来の手法では十分に解決出来なかった論理的推論の課題に対し、人間の思考プロセスの研究から得られた知見を活用するアプローチである。

</details>

# 1/13(Mon)
<details><summary>GPT-4o、Gemini、Claude 3などにおける「長いプロンプトのマルチモーダルタスク」性能を測定した結果</summary>

マルチモーダル大規模言語モデル(MLLM)は進歩しており、ベンチマークで優れたパフォーマンスを発揮しているが、ベンチマークの範囲が限られているため、現実世界、長いコンテキスト、および複数の画像のタスクにおける有効性は不明である。既存のベンチマークは、多くの場合、単一画像と短いテキストのサンプルに焦点を当てており、複数画像のタスクを評価する場合は、画像数を制限するか、特定のタスク(時系列のキャプションなど)に焦点を当てているため、MLLMのパフォーマンス上の課題が不明瞭になる可能性がある。これらの制限に対処するために、MLLMのマルチモーダルロングコンテキスト機能をテストするように設計された先駆的なベンチマークであるMileBenchを紹介する。このベンチマークは、マルチモーダルロングコンテキストだけでなく、理解と生成の両方を必要とする複数のタスクで構成されている。診断用と現実的という2つの異なる評価セットを確率して、MLLMのロングコンテキスト適応能力と、ロングコンテキストシナリオでタスクを完了する能力を体系的に評価する。22のモデルをテストして得られた実験結果から、クローズドソースのGPT-4oは他のモデルよりも優れているものの、ほとんどのオープンソースのMLLMはロングコンテキストの状況で苦戦していることが明らかになった。興味深いことに、画像の数が増えるとパフォーマンスのギャップが広がる傾向がある。特に複数の画像が関係するシナリオでは、MLLMのロングコンテキスト機能を強化するための研究努力を強化することを強くおすすめする。

</details>

# 1/20(Mon)
<details><summary>LLMの推論能力は単純に文脈を繰り返すだけでも大幅に向上　最大で30%改善</summary>
  
**背景**  
複数の文書を参照しながら段階的に推論を進める「多段階推論」と呼ばれるタスクは、LLMにとってまだ難しい課題となっている。質問に関連する複数の文書から役立つ情報を探し出し、それらを組み合わせて答えを導き出す必要があるタスクである。  
多段階推論では、LLMはいくつかの課題に直面している。まず、答えに関係のない情報をうまく除外できないことがある。また、文章の位置によって推論の精度が大きく変わってしまうという問題もある。実際、長い文脈の中央部分にある情報は、モデルに見落とされてしまうことがある。  
さらに今回の研究者らは、「文書が提示される順序によってLLMの推論性能が大きく変わってしまう問題」にも新たに注目している。例えば、同じ内容の文書でも、提示順序を変えるだけで回答の正確さが変わってしまう。  
この「文書の順序による問題」は、実際のタスクでは避けられない課題である。なぜなら、LLMに最適な順序で文書を提示できる保証がないからである。   
そこで研究者らは、この問題を解決するための新しい方法を提案している。与えられた文脈(複数の文書)を繰り返し提示することで、LLMが最適な順序で文書を理解できるようにする。

[参考](https://arxiv.org/pdf/2410.07103?)

</details>

# 1/27(Mon)
<details><summary>LLMの推論能力を戦略的に向上させる新しいプロンプト手法「SCoT」</summary>

  
SCoTは、まず問題を解く作戦を考え、作戦をベースとして考え方の道筋を作るといった２段階からなるアプローチである。研究グループは、様々な課題でSCoTがうまくいくかを試し、通常のCoTと比べて多くの課題でより良い結果が出ることを明らかにする。この研究は、LLMの考える力をさらに高められる可能性を示している。難しい課題にLLMを使う際の知見になるかもしれない。  
**背景**  
LLMの考える力を高めるプロンプト手法としてCoT（思考の連鎖）が注目されている。CoTはLLMに考え方の手順を示すことで、難しい問題を解けるようにするアプローチである。しかし、CoTには問題点がある。同じ問題でも、解き方によって答えの正確さが変わってしまうことがある。つまり中間ステップが間違っていたらその先の答えがやはり間違ってしまうという問題である。  
CoTの弱点を補強するために、これまで様々な方法が試されてきた。例えば、複数の解き方を試して、一番信頼できる答えを選ぶ方法や、外部の知識を使って考え方を強くする方法などである。しかし、計算に時間がかかりすぎたり、専門家の知識が必要だったりするアプローチは、実際に使うのが難しい。  
そこで今回新しいプロンプト手法「SCoT(Strategic Chain-of-Thought)」が提案された。SCoTは、「１回の指示で２つのことを行う」ことがポイントのプロンプトフレームワークである。まず、問題を解くための最適な戦略を考え出し、次にその戦略を使って質の高い「考え方の道筋」を作り、最終的な答えを導きだす。新手法SCoTの良いところは、質問を複数回行う必要なく、外部の知識を必要とせず、１回の指示で効果的に考えられる点である。計算にかかる時間やコストを抑えつつ、LLMも推論能力を高められると期待されている。さらに、SCoTの文脈内学習（プロンプト内に例を含んで学習させるプロンプト手法）に発展させ、最適な例を自動的に見つける方法も開発された。実験によると、これでさらに性能が向上している。実験では、数学、常識、物理、空間、多段階の考え方など、様々な分野の８つのデータセットが使用された。その結果、多くのモデルで大きな性能向上が確認された。

[参照](https://arxiv.org/pdf/2409.03271?)

</details>

<details><summary>なぜDeepSeek V3は"破壊的"なのか？</summary>

 
- わずか数ヶ月/約558万ドルという圧倒的低コストで学習完了
- GPT-4oに匹敵する性能をうたうオープンソースモデルであり、APIも数分の1~十数分の１と圧倒的価格破壊を起こしている
- Mixture-of-Experts(MoE)を活用して370B級モデルを安価/高速に動作

  
**１. 圧倒的なコストパフォーマンスがもたらす衝撃**  
| モデル名             | コンテキスト長         | 入力価格 (/1M tokens)                         | 出力価格 (/1M tokens) | 特徴 / 備考                                            |
|-----------------------|------------------------|-----------------------------------------------|-----------------------|-----------------------------------------------------|
| DeepSeek V3          | 64K (最大出力8K)       | - キャッシュヒット: $0.014<br>- キャッシュミス: $0.14 | $0.28                | - Mixture-of-Experts (MoE)<br>- 低コストで割引中<br>- オープンソース (MITライセンス) |
| GPT-4o               | 128K                  | $2.50                                        | $10.00               | - GPT-4派生モデル (Vision対応)<br>- 高性能だが価格も高め |
| GPT-4o mini          | 128K (推定)           | $0.15                                        | $0.60                | - GPT-4oの小型・低コスト版 (Vision対応)<br>- 軽量実装ながら高精度 |
| Claude 3.5 Sonnet    | 200K                  | $3.00                                        | $15.00               | - 長いコンテキストに強い<br>- Anthropic提供<br>- 文章解析や会話性能に定評 |
| Gemini 1.5 Flash     | 1M (推定)             | $0.15                                        | $0.30                | - Google提供 (Gemini系)<br>- 小規模プロジェクトに対応可能<br>- 高速応答 |
| Gemini 1.5 Pro       | 2M (推定)             | $3.50                                        | $10.50               | - 商用アプリ向け<br>- 1,000リクエスト/分程度の高トラフィック対応 |

実際の個人開発レベルでLLMを組み込もうとするとすると、どうしてもコスト面から4o-miniやgemini1.5flashが選ばれがちである。しかし品質という観点でいくならばまだまだ伸び代を感じるのが正直な話である。  

## わずか2か月・約558万ドルで学習
DeepSeek社によれば、DeepSeek V3は約2か月間で学習を完了し、その費用は約558万ドル(6億円弱)とされている。OpenAIがGPT-4開発に”数十億ドル”、MetaがLlama 2に”数億ドル”規模を投じたといわれるなかで、この数字は破格である。

## 大規模パラメータ+MoEによる効率化
DeepSeek V3は**6710億パラメータ**を持つ超大規模LLMであるが、実際に推論に使われるのは**370億パラメータ相当**となるよう設計されている。これは「Mixture-of-Experts（MoE）」という技術により、トークンごとに必要な”専門家”モジュールだけを活性化させる仕組みのおかげ。膨大な総パラメータを抱えながらも、実行にかかるリソースを抑えることに成功している。

## GPT-4o級性能をオープンソース＋激安APIで
<img width="398" alt="image" src="https://github.com/user-attachments/assets/5140fb3a-c738-4f28-bdbe-6e5ebccd4f06" />  

DeepSeek社自身のベンチマークや外部の初期検証によると、数学タスク・プログラミングタスクなど多方面でGPT-4oに肉薄あるいは同等以上の性能を示すケースが報告されている。さらにDeepSeek V3はMITライセンスの下、オープンソースとして公開されているため、自由にカスタマイズや再配布が可能である。

## 2. 具体的にどんなタスクに使えるのか？
DeepSeek V3をはじめとするLLMが一般的に得意とするタスクは、多岐にわたる。しかし、このモデルが”破壊的”なのは、大掛かりなGPUリソースを必ずしも必要とせず、比較的少ない予算でこれらのタスクを実現できる点にある。

## 2-1. RAG（Retrieval-Augmented Generation）による高度な情報検索
DeepSeek V3は**128Kトークン相当**のコンテキスト長をサポートしている。これは、さらにRAG（Retrieval-Augmented Generation）という手法を組み合わせれば、事前に文書データを検索エンジンなどでピンポントに取り出してから生成を行うことが可能である。  
こうすることで、大量のテキストを無理やりモデルに食わせる必要がなくなり、トークンの浪費を抑える効率的な応答が期待できる。

## 2-2. 感情評価や書評などの"定性処理"
生成AIの強みは、定量的な数値計算だけでなく、人間が読む文書のトーンや感情、意図を理解し、それに対する評価や所感を生成できる点にある。たとえば、
- 書籍の内容を要約しつつ、感想や書評を付け加える
- SNSの投稿や製品レビューからユーザーの感情を分析する
- 映画のストーリーラインからテーマを抽出する
などの"定性処理"も、DeepSeek V3であれば低コストに実行できるでしょう。

## 2-3. チャットボット・ＱＡシステム
ユーザーとの対話形式で情報をやり取りするチャットボットやQAシステムは、LLMの代表的なユースケースの一つである。DeepSeek V3が持つ大容量のコンテキスト処理は、長い会話履歴や複雑な問い合わせへの対応に向いており、ドキュメント検索×QAにも相性が良い。

## 2-4. コード生成やデバッグ支援
コーディングタスクに強いことも、DeepSeek V3の特徴として挙げられる。ログイン機能や機械学習アルゴリズムのサンプルコード、さらにはバグ修正や最適化のヒントなど、開発現場での「もう少し詳しいアドバイスがほしい」というニーズにもこたえてくれる可能性がある。

## 3. コスト比較のイメージ：本当に安いのか？
OpenAIのGPTシリーズやAnthropicのClaudeシリーズと比べても、**数分の1~10分の1程度の推論コスト**で利用でき、特にリリース直後は割引価格が適用されるためさらに導入しやすくなっている。  

たとえばDeepSeek公式サイトのAPI料金表を見ると、入力・出力ともに**100万トークンあたり数十セント~数ドル**という水準で提供されており、GPT-4oの数分の1に収まるケースもある。  
もちろん利用形態や負荷量によって最適な選択は異なるが、「高性能LLMをリーズナブルに試したい」というニーズには、まさに"破壊的"な価格設定といえる。

</details>

# 1/28(Tu)
<details><summary>LLMには正解例だけでなく、「よくある間違い例」と理由も一緒に教えるのが有効</summary>


## 背景
LLMに「考えるステップを示す」というテクニック（Chain-of-Thought、以下CoT）が注目されている。例えば数学の問題を解くときに、「まずこう考えて、次にこうして...」というように、考え方の手順を示すものである。  
このCoTが効果的だということは実験で分かってきたが、なぜうまくいくのか、その仕組みはわかっていなかった。  
これまでの研究では、考えるステップを「バラバラに切り離して」分析していた。つまり、「ステップ1→ステップ2→ステップ3」という流れを、それぞれ独立した部分として扱っていた。  
しかし、実際のLLMは前のステップの内容を覚えていて、それを踏まえて次のステップを考えている。そこで「各ステップのつながりを大切にした分析が必要なのでは？」という問題意識が生まれました。  
また、CoTを使うときに「途中のステップで間違えると最終的な答えにどれくらい影響するのか？」という疑問もある。例えば、数学の問題で途中の計算を間違えた場合と、最後の答えだけ間違えた場合では、どちらが深刻な影響を及ぼすのだろうか。  

このような疑問があったため、研究チームは以下の2つを明らかにしようと考えた。

1. ステップ同士のつながりを考慮したほうが、バラバラに分析するよりも良い結果が得られるのか
2. 途中のステップでの間違いと、最後の答えでの間違いでは、どちらがより大きな影響を与えるのか

つまり、「考えるプロセスの一貫性」と「間違いの影響度」を理論的に解明しようとした。

[参考](https://arxiv.org/pdf/2410.16540)


## 中国ディープシーク、米オープンAIからデータを不正入手か....
突如出現したDeepSeekのAIモデル「R1」は、はるかに少ないコストやリソースしか使っていないにもかかわらず、OpenAIやGoogleなどの大手企業と同等の成果を達成したように見えた。そして、そのせいで世界ぼ金融市場はAIに注力しているテクノロジー企業に対する疑問をふくらませることとなり、特にAI企業に大量のGPUを納入しているNvidiaの株価が、同社史上最大の下落を記録したことが大きなニュースになった。  
一方、ポッと出のAIベンチャーにお株を奪われた格好になった
OpenAIとマイクロソフトは、DeepSeekがOpenAIの製品からデータを違法に「抽出」し、別のAIモデルをトレーニングするために使った可能性を疑っている。それはもちろん、OpenAIの利用規約で禁止されている行為である。ただ、DeepSeekがOpenAIのAIモデルを利用したことを示す「相当な証拠」があるとFox Newsに述べ「今後数ヶ月のうちに、主要なAI企業が「抽出」防止策を講じることになるだろう。そうなると、このような模倣モデルのいくつかは間違いなく勢いを失うだろう」とした。  
Financial Timesは、OpenAIがすでにOpenAIがDeepSeekによる「抽出」の証拠をいくつか発見したと述べている。  
皮肉なことを言えば、OpenAI自身(や、多くのAI企業)も、インターネット上に公開されているニュース記事などのデータを許可なくAI強化に使用しており、New York Timesをはじめとするいくつものメディア企業から訴訟を起こされており、因果応報と言えるかもしれない。

このような問題も抱えているため、むやみやたりに利用することは危険である。
</details>

# 1/29(Wed)
<details><summary>興津テスト、AIの得点率91%　東大文1のボーダー超える</summary>
  
| 科目                     | 得点  |
|------------------------|------|
| 国語                     | 94%  |
| 英語リーディング       | 98   |
| 英語リスニング         | 93   |
| 数学 I A               | 80   |
| 数学 II B C           | 93   |
| 歴史総合、世界史探究  | 97   |
| 歴史総合、日本史探求  | 79   |
| 地学基礎           | 92    |
| 生物基礎           | 94    |
| 情報　Ⅰ           | 92    |
| 平均              | 約91%  |

大手予備校の河合塾が昨年11月に公表した、合格の可能性が50%￥になると予想した「ボーダーライン」の得点率(86%)を超えた。  
AIが学習するインターネット上のデータ量は、日本史が世界史と比べて少ないことが影響した結果、得点率が日本史に関して低くなったと考えられる。  
ライフプロンプトは対話型AI「チャットGPT」を開発した、米オープンAIの最新モデル「o1」を使った。共通テストの問題文を画像データにした上で、AIの質問欄に入力して解答を求めた。

</details>

# 1\30(Th)
<details><summary>実在する人間1052人の態度と行動をAIでモデル化インタビューベースのエージェントが人間の解答を85%再現</summary>

人間の行動をシミュレーションできるエージェントの開発は、様々な可能性を秘めている。例えば、新しい公衆衛生のルールに対する人々の反応、新製品が発売された時の市場の動き、予期せぬ出来事への人々の対応などを事前に知ることなどが期待される。また、社会の仕組みや組織、人々のつながりをより深く理解するのにも役立つかもしれない。  
これまでのエージェントシミュレーションの取り組みでは、エージェントは人が作ったルールや数式に基づいて動きている。これはわかりやすい反面、現実の人間のように状況に合わせて複雑な行動をとることが苦手といった課題があった。しかしLLMが登場し、様々な状況での人間の行動をより正確にシミュレーションできるようになる可能性が開かれた。  
ただし、LLMを使用したエージェントシミュレーションを単純に使用するだけでは、どうしても「こういうタイプの人ならこう行動する」といった固定観念に陥りがちである。また、シミュレーション結果が、「だいたいあってる」かどうかを判断するだけでなく、もっと細かく分析できる方法が必要である。  
そこで、スタンフォード大学の研究者を中心としたチームは、1,000人以上の人々に2時間かけて行った詳しいインタビューのデータとLLMを組み合わせて、個人の考え方や行動を再現するエージェントを開発した。

</details>

<details><summary>エージェントなしで行うLLMによるソフトウェアのバグ修正手法</summary>

近年の大規模言語モデル(LLM)の進展により、コード生成やプログラム修正、テスト生成などのソフトウェア開発タスクの自動化が大きく進歩している。特に、LLMを活用した自律的なソフトウェアエージェントが、ツールの使用やコマンドの実行、環境からのフィードバックの観察、将来の行動計画といった能力を備え、エンドツーエンドのソフトウェア開発タスクを実行するために開発されている。  

しかし、これらのエージェントベースのアプローチの複雑さや、現在のLLMの能力の限界を考慮すると、「本当に複雑な自律的ソフトウェアエージェントを採用する必要があるのか？」という疑問が生じる。この問いに答えるために、著者らは「AGENTLESS」というエージェントを用いないアプローチの冗長で複雑な設定とは対称的に、ローカリゼーション、修正、パッチ検証の3つのフェーズからなるシンプルなプロセスを採用しており、LLMが将来の行動を決定したり、複雑なツールを操作したりすることはない。  

著者らは実験結果によれば、AGENTLESSは、人気のあるSWE-bench Liteベンチマークにおいて、既存のオープンソースのソフトウェアエージェントと比較して、パフォーマンス(32.00%、96の正しい修正)と低コスト($0.70)を達成した。さらに、AGENTLESSは、GPT-4oや新しいOpenAI o1モデルの実際のコーディング性能を示すためのアプローチとして、OpenAIによって採用されている。  

この研究は、自律的なソフトウェア開発におけるシンプルでコスト効果の高い手法の可能性を強調しており、今後の研究に新たな視点を提供する。

(参考)[https://arxiv.org/pdf/2407.01489]

</details>
