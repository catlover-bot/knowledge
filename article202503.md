# 3/17  
<details><summary>AIコーディングアシスタント「Cursor」が4ヶ月の短期間で1億ドル調達　加熱するコーディングAI開発競争</summary>

AIコーディングアシスタント「Cursor(カーソル)」を開発するAnysphereが約１億ドルの資金調達を実施し、評価額が26億ドルに達したことが明らかになった。  
これは、わずか４ヶ月前の４億ドルから、6.5倍という急激な評価額の上昇である。Cursorは、OpenAI、Midjourney、Shopifyなど著名企業も採用する人気ツールで、収益も急成長を遂げている。  

開発者向けAIアシスタント市場では、マイクロソフトのGitHub Copilotも無料版をリリースするなど、多くのプレイヤーが競争を繰り広げているが、そのなかでも一際注目を集めているCursorの強み・特徴えお分析しつつ、急成長の背景を探っていく。  
### 盛り上がるAIコーディングツールの市場で注目される「Cursor」  
AIコーディングツールの市場の成長は著しく、米国のマーケット調査会社Polaris Researchによると、2032年までに271億7,000万ドルの規模に達すると予想されており、GitHubによる最新の開発者アンケートでは、回答者の大多数が何らかの形でAIツールを導入していると答えている。  

中でも人気のCursorを開発するAnysphereは、マサチューセッツ工科大学の学生だったマイケル・トレーエル氏らが2022年に設立したスタートアップである。同社は、OpenAIのアクセラレータープログラムを経て急成長を遂げ、40.000社を超える顧客を抱える企業へと成長した。  

2024年4月時点で年間400万ドルだった収益は、10月には月間400万ドル(年換算4,800万ドル)にまで拡大。昨年11月には、AIコーディングアシスタント「Supermaven」を非公開の金額で買収し、さらなる躍進を目指している。  

### Cursorの強みはそのシンプルさ  
Cursorが目指しているのは、複雑なプログラミングをよりシンプルかつ効率的に実現可能にすることである。  

主な特徴は、簡潔な指示を解釈して実用的なコードスニペット(プログラミング言語の中で切り貼りして再利用できるコード)などに変換し、外部から見た時の挙動は変えずに、理解や修正がしやすいようにプログラムの内部構造を整理する「コードリファクタリング」を数秒で実行する機能である。  

すでに使用しているツールやフレームワークとも簡単に統合できるようになっており、この互換性により、既存のワークフローに大きな変更を加えることなく、AIツールの導入ができることもメリットである。  

料金体系もシンプルで、２週間の無料トライアル後、プロプランが月額20ドル、ビジネスプランが月額40ドルとなっている。  

### 「Tab」キー連打でコーディング  
シンプルさを強調するCursorの謳い文句は、「Tab」キーの連打でコーディングできる、というものである。コードを入力すると、AIが続きのコードを提案し、「Tab」キーをクリックしていくことで、次々とAIによって瞬時に生成されるコードが後に続いていく。  

OpenAIの共同設立者であり、テスラのAiディレクターとしても知られるアンドレイ・カーパシー氏はXで、「Future be like tab tab tab」とツイートし、「コーディングの未来はTab連打」と、Cursorの使用感を伝えた。  

Cursorで使用するAIは、初期から利用されていたGPT-4/GPT-4oに加え、現在は、コーディングが高速で正確であると評判のClaude 3.5 Sonnet LLMも任意で選択可能である。  

### 汎用性の高さと高速なコード補完のCodeium  
Aiコーディングアシスタントの中では、昨年の資金調達で1億5,000万ドルを調達し、評価額が12億5,000万ドルに達したユニコーン企業、Codeiumも注目株である。  

コード関連タスクに最適化された独自開発の大規模言語モデル(LLM)を活用したCodeiumのプラットフォームは、高速なコード提案やエラー検出、コードの自動最適化の提供をすることで、ソフトウェア開発の効率化を図ることができる。  

Codeiumは汎用性の高さに強みがあり、70を超えるプログラミング言語をサポートしていることに加えて、40を超える統合開発環境(IDE)とシームレスに統合することができる。  

### 開発者を堅実にサポートするAugment  
一方、2024年4月に2億2,700万ドルを調達、総調達額が2億5,2000万ドルへと達し、ユニコーン企業まであと一歩の評価額9億7,700万ドルとなっているのが、同じくカリフォルニア発のAugmentである。  

AugmentのAIコーディングアシスタントは、リアルタイムでの高度なエラー検出や修正案の提案、コード内の脆弱性を検出しセキュリティを強化、また大規模な開発者チームに向け、共同ワークフローを最適化するような機能も備えているなど、開発者を多方面からサポートする堅実なアプローチに定評がある。Slackなど外部チームワークコミュニケーションサービスとの連携も可能である。  

### AIコーディングによる新たな課題や負担も  
もっとも、他の分野でのAIツールと同じように、AIコーディングへの現場からの評価はいまだ厳しいものである。  

サンフランシスコのAI企業Harnessから発表された500人のソフトウェアエンジニアを対象とした調査によると、95%以上がAiツールがエンジニアの燃え尽き症候群を軽減できると好意的に受け止めている一方で、半数以上(59%)がAI生成コードがエラーを引き起こしていること、また回答者の92%が、AIツールによってデバッグが必要なコードが影響を及ぼす範囲が拡大していると回答した。  

また、3分の2以上の回答者が、AI生成コードのデバッグやAI関連のセキュリティ脆弱性の解決に人間が多くの時間を費やしていると指摘した。  

これは、開発者が自身のコードのデバッグより時間がかかるとされる「自分が作成に関与していないコードのデバッグ」に時間をとられているためではないかと指摘されており、AIツールの導入が効率化をもたらす一方で、新たな課題や負担を開発者に課している現状が浮き彫りになっている。
</details>

<details><summary>Google Researchが新たな科学研究ツール「AI共同科学者」を発表</summary>

Googleは、同社の生成AI「Gemini 2.0」を使用して構築されたシステム「AI co-scientist」（AI共同科学者）を発表した。  

研究者が自然言語で研究目標を指定すると、AI co-scientistが仮説、研究概要、実験プロトコルなどを生成する。生成されたものに対して、研究者が自然言語でフィードバックすることなども可能。  

AI co-scientistは、調査収集と作業の洗練において研究者を支援するツールであり、科学的プロセスを自動化するものではないという。  

Trusted Testerプログラムに参加している研究者は、AI co-scientistに早期アクセスできるようになる。  
</details>

# 3/18(Tu)  
<details><summary>LLMの革新、トークンからバイトへ　メタが開発した新アーキテクチャ「BLT」の全容</summary>

### 大規模言語モデル開発の課題；トークン利用における非効率性  
AI研究コミュニティは、大規模言語モデル（LLM）の新たな改善方法を模索し続けている。  

特に注目される課題の1つとして、既存LLｍが依拠するトークンベースアーキテクチャにおける非効率性が挙げられる。  

そもそも、LLMの文脈における「トークン」とは、事前に定義されたバイト（データの最小単位）の組合わせのことを指す。LLMは、入力テキストをこのトークンに分解して処理する。これにより、計算リソースを効率的に使用することが可能になる。  

例えば「intelligence」という単語を考えてみる。コンピュータ上では、この単語は「i」「n」「t」「e」「l」「l」「l」「i」「g」「e」「n」「c」「e」という12個の文字（バイト）として保存されている。しかし、LLMはこの単語全体を「intelligence」という1つのトークンとして扱うことができる。これは、人間が文字を1つずつ読むのではなく、「intelligence」を1つの意味のある単位として瞬時に認識するのと似た仕組みである。このおうに単語やよく使われる文字の組合わせを1つのトークンとして扱うことで、LLNはテキストをより効率的に処理できるようになる。  

しかし、このトークンベースのアプローチには、いくつかの重大な課題が存在する。その1つが、固定された語彙に起因する処理の偏りである。特にウェブ上での出現頻度が低い言語を処理する際、その言語の単語が語彙に含まれていないために、処理が遅くなったり、コストが増大したりする問題が発生する。  

たとえば「computer」という単語は1つのトークンとして処理できるが、ウェブ上で出現頻度の低い言語の単語は、「co」「mp」「ut」「er」のように複数の小さなトークンに分割して処理せざるを得ない場合がある。これは、その言語の単語が事前に定義された語彙に含まれていないため起こる。このような分割処理は、計算コストの増加や処理速度の低下、さらには精度の低下にもつながる可能性がある。  

また、スペルミスへの対応も大きな課題となっている。入力テキストに誤字があった場合、モデルが不適切なトークン分割を行う可能性があり、結果として処理制度が低下する。さらに、文字レベルのタスク、たとえばン文字列の操作などにおいても、トークンベースのモデルは苦手とする傾向にあるとされる。  

トークン語彙の修正や拡張にも大きな制約がある。語彙を変更するには、モデルの再学習が必要となる。またトークン語彙を拡張する場合、モデルのアーキテクチャ自体の変更が必要となり、追加された複雑性に対応するための調整が求められる。  

代替案として、LLMを単一のバイトで直接学習させる方法も考えられる。これにより、上述した多くの問題を解決できる可能性がある。しかし、こおｎ方法にも大きな課題がある。バイトレベルのLLMは、大規模なモデルを学習させるためのコストが法外に高く、また非常に長いシーケンスを処理することができない。これが、現在のLLMにおいてトークン化が必須のプロセスとして残されている主な理由である。  


</details>
