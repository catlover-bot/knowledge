# 3/17  
<details><summary>AIコーディングアシスタント「Cursor」が4ヶ月の短期間で1億ドル調達　加熱するコーディングAI開発競争</summary>

AIコーディングアシスタント「Cursor(カーソル)」を開発するAnysphereが約１億ドルの資金調達を実施し、評価額が26億ドルに達したことが明らかになった。  
これは、わずか４ヶ月前の４億ドルから、6.5倍という急激な評価額の上昇である。Cursorは、OpenAI、Midjourney、Shopifyなど著名企業も採用する人気ツールで、収益も急成長を遂げている。  

開発者向けAIアシスタント市場では、マイクロソフトのGitHub Copilotも無料版をリリースするなど、多くのプレイヤーが競争を繰り広げているが、そのなかでも一際注目を集めているCursorの強み・特徴えお分析しつつ、急成長の背景を探っていく。  
### 盛り上がるAIコーディングツールの市場で注目される「Cursor」  
AIコーディングツールの市場の成長は著しく、米国のマーケット調査会社Polaris Researchによると、2032年までに271億7,000万ドルの規模に達すると予想されており、GitHubによる最新の開発者アンケートでは、回答者の大多数が何らかの形でAIツールを導入していると答えている。  

中でも人気のCursorを開発するAnysphereは、マサチューセッツ工科大学の学生だったマイケル・トレーエル氏らが2022年に設立したスタートアップである。同社は、OpenAIのアクセラレータープログラムを経て急成長を遂げ、40.000社を超える顧客を抱える企業へと成長した。  

2024年4月時点で年間400万ドルだった収益は、10月には月間400万ドル(年換算4,800万ドル)にまで拡大。昨年11月には、AIコーディングアシスタント「Supermaven」を非公開の金額で買収し、さらなる躍進を目指している。  

### Cursorの強みはそのシンプルさ  
Cursorが目指しているのは、複雑なプログラミングをよりシンプルかつ効率的に実現可能にすることである。  

主な特徴は、簡潔な指示を解釈して実用的なコードスニペット(プログラミング言語の中で切り貼りして再利用できるコード)などに変換し、外部から見た時の挙動は変えずに、理解や修正がしやすいようにプログラムの内部構造を整理する「コードリファクタリング」を数秒で実行する機能である。  

すでに使用しているツールやフレームワークとも簡単に統合できるようになっており、この互換性により、既存のワークフローに大きな変更を加えることなく、AIツールの導入ができることもメリットである。  

料金体系もシンプルで、２週間の無料トライアル後、プロプランが月額20ドル、ビジネスプランが月額40ドルとなっている。  

### 「Tab」キー連打でコーディング  
シンプルさを強調するCursorの謳い文句は、「Tab」キーの連打でコーディングできる、というものである。コードを入力すると、AIが続きのコードを提案し、「Tab」キーをクリックしていくことで、次々とAIによって瞬時に生成されるコードが後に続いていく。  

OpenAIの共同設立者であり、テスラのAiディレクターとしても知られるアンドレイ・カーパシー氏はXで、「Future be like tab tab tab」とツイートし、「コーディングの未来はTab連打」と、Cursorの使用感を伝えた。  

Cursorで使用するAIは、初期から利用されていたGPT-4/GPT-4oに加え、現在は、コーディングが高速で正確であると評判のClaude 3.5 Sonnet LLMも任意で選択可能である。  

### 汎用性の高さと高速なコード補完のCodeium  
Aiコーディングアシスタントの中では、昨年の資金調達で1億5,000万ドルを調達し、評価額が12億5,000万ドルに達したユニコーン企業、Codeiumも注目株である。  

コード関連タスクに最適化された独自開発の大規模言語モデル(LLM)を活用したCodeiumのプラットフォームは、高速なコード提案やエラー検出、コードの自動最適化の提供をすることで、ソフトウェア開発の効率化を図ることができる。  

Codeiumは汎用性の高さに強みがあり、70を超えるプログラミング言語をサポートしていることに加えて、40を超える統合開発環境(IDE)とシームレスに統合することができる。  

### 開発者を堅実にサポートするAugment  
一方、2024年4月に2億2,700万ドルを調達、総調達額が2億5,2000万ドルへと達し、ユニコーン企業まであと一歩の評価額9億7,700万ドルとなっているのが、同じくカリフォルニア発のAugmentである。  

AugmentのAIコーディングアシスタントは、リアルタイムでの高度なエラー検出や修正案の提案、コード内の脆弱性を検出しセキュリティを強化、また大規模な開発者チームに向け、共同ワークフローを最適化するような機能も備えているなど、開発者を多方面からサポートする堅実なアプローチに定評がある。Slackなど外部チームワークコミュニケーションサービスとの連携も可能である。  

### AIコーディングによる新たな課題や負担も  
もっとも、他の分野でのAIツールと同じように、AIコーディングへの現場からの評価はいまだ厳しいものである。  

サンフランシスコのAI企業Harnessから発表された500人のソフトウェアエンジニアを対象とした調査によると、95%以上がAiツールがエンジニアの燃え尽き症候群を軽減できると好意的に受け止めている一方で、半数以上(59%)がAI生成コードがエラーを引き起こしていること、また回答者の92%が、AIツールによってデバッグが必要なコードが影響を及ぼす範囲が拡大していると回答した。  

また、3分の2以上の回答者が、AI生成コードのデバッグやAI関連のセキュリティ脆弱性の解決に人間が多くの時間を費やしていると指摘した。  

これは、開発者が自身のコードのデバッグより時間がかかるとされる「自分が作成に関与していないコードのデバッグ」に時間をとられているためではないかと指摘されており、AIツールの導入が効率化をもたらす一方で、新たな課題や負担を開発者に課している現状が浮き彫りになっている。
</details>

<details><summary>Google Researchが新たな科学研究ツール「AI共同科学者」を発表</summary>

Googleは、同社の生成AI「Gemini 2.0」を使用して構築されたシステム「AI co-scientist」（AI共同科学者）を発表した。  

研究者が自然言語で研究目標を指定すると、AI co-scientistが仮説、研究概要、実験プロトコルなどを生成する。生成されたものに対して、研究者が自然言語でフィードバックすることなども可能。  

AI co-scientistは、調査収集と作業の洗練において研究者を支援するツールであり、科学的プロセスを自動化するものではないという。  

Trusted Testerプログラムに参加している研究者は、AI co-scientistに早期アクセスできるようになる。  
</details>

# 3/18(Tu)  
<details><summary>LLMの革新、トークンからバイトへ　メタが開発した新アーキテクチャ「BLT」の全容</summary>

### 大規模言語モデル開発の課題；トークン利用における非効率性  
AI研究コミュニティは、大規模言語モデル（LLM）の新たな改善方法を模索し続けている。  

特に注目される課題の1つとして、既存LLｍが依拠するトークンベースアーキテクチャにおける非効率性が挙げられる。  

そもそも、LLMの文脈における「トークン」とは、事前に定義されたバイト（データの最小単位）の組合わせのことを指す。LLMは、入力テキストをこのトークンに分解して処理する。これにより、計算リソースを効率的に使用することが可能になる。  

例えば「intelligence」という単語を考えてみる。コンピュータ上では、この単語は「i」「n」「t」「e」「l」「l」「l」「i」「g」「e」「n」「c」「e」という12個の文字（バイト）として保存されている。しかし、LLMはこの単語全体を「intelligence」という1つのトークンとして扱うことができる。これは、人間が文字を1つずつ読むのではなく、「intelligence」を1つの意味のある単位として瞬時に認識するのと似た仕組みである。このおうに単語やよく使われる文字の組合わせを1つのトークンとして扱うことで、LLNはテキストをより効率的に処理できるようになる。  

しかし、このトークンベースのアプローチには、いくつかの重大な課題が存在する。その1つが、固定された語彙に起因する処理の偏りである。特にウェブ上での出現頻度が低い言語を処理する際、その言語の単語が語彙に含まれていないために、処理が遅くなったり、コストが増大したりする問題が発生する。  

たとえば「computer」という単語は1つのトークンとして処理できるが、ウェブ上で出現頻度の低い言語の単語は、「co」「mp」「ut」「er」のように複数の小さなトークンに分割して処理せざるを得ない場合がある。これは、その言語の単語が事前に定義された語彙に含まれていないため起こる。このような分割処理は、計算コストの増加や処理速度の低下、さらには精度の低下にもつながる可能性がある。  

また、スペルミスへの対応も大きな課題となっている。入力テキストに誤字があった場合、モデルが不適切なトークン分割を行う可能性があり、結果として処理制度が低下する。さらに、文字レベルのタスク、たとえばン文字列の操作などにおいても、トークンベースのモデルは苦手とする傾向にあるとされる。  

トークン語彙の修正や拡張にも大きな制約がある。語彙を変更するには、モデルの再学習が必要となる。またトークン語彙を拡張する場合、モデルのアーキテクチャ自体の変更が必要となり、追加された複雑性に対応するための調整が求められる。  

代替案として、LLMを単一のバイトで直接学習させる方法も考えられる。これにより、上述した多くの問題を解決できる可能性がある。しかし、こおｎ方法にも大きな課題がある。バイトレベルのLLMは、大規模なモデルを学習させるためのコストが法外に高く、また非常に長いシーケンスを処理することができない。これが、現在のLLMにおいてトークン化が必須のプロセスとして残されている主な理由である。  

### メタの研究者らが発表したトークンに依拠しないアプローチ、その概要  

こうした課題に対し、メタとワシントン大学の研究者らが画期的な解決策を提示した。それが新しいアーキテクチャ「[Byte Latent Transformer(BLT)](https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/)」である。  

BLTは、トークナイザーを使用せずに生のバイトデータから直接学習できる初のアーキテクチャとして注目を集めている。  

BLTの中核となるのは、バイトを動的にパッチにグループ化する手法である。このアプローチでは、データの複雑さに応じて計算リソースを柔軟に配分することが可能となる。例えば、単語の末尾部分のように予測が比較的容易な部分には少ないリソースを割り当て、文の最初の単語など、予測が困難な部分により多くの計算リソースを配分する。  

アーキテクチャは3つのブロックで構成されている。2つの軽量なバイトレベルのローカルモデル（エンコーダー/デコーダー）と、1つの大規模な「潜在グローバルトランスフォーマー」である。エンコーダーは入力バイトをパッチ表現に変換し、デコーダーはパッチ表現を生のバイトに戻す役割を担う。そして、グローバルトランスフォーマーが学習と推論の主要な処理を行う。  

![image](https://github.com/user-attachments/assets/3e7bbc66-0324-40b6-a3d2-5c9270481e77)  

これは、多言語の会議での通訳システムのようなものといえるだろう。エンコーダーは、参加者の発言（入力データ）を一定のまとまり（パッチ）に整理して、会議の共通言語（パッチ表現）に変換する通訳者の役割を果たす。グローバルトランスフォーマーは、その共通言語で行われる会議の本体であり、実際の議論や意思決定（主要な処理）を行う。そして、デコーダーは、会議での決定事項を再び各参加者の言語（出力データ）に翻訳して伝える通訳者の役割を担う。このように3つの要素が連携することで、効率的な情報処理を実現している。  

一方、従来のLLMは、事前に定義された固定の辞書（トークナイザー）を使用する仕組みである。この会議の例でいえば、全ての参加者が同じ辞書を使って発言を定型的な方言に変換してから会議に参加するようなものである。この方法は効率的である一方、辞書に載っていない表現や新しい言い回しに対応できないという制約がある。これに対しBLTは、入力される情報の特性に応じて柔軟に処理方法を変えることができ、より自然な言語処理を実現できる。  

この新しいアプローチの特筆すべき点は、従来のトークンベースのモデルと同等のパフォーマンスを達成しながら、推論効率を大幅に改善できる点にある。研究チームの実験によると、BLTはLlama 3と同等の学習西欧を示しながら、推論時のFLOP（浮動小数点演算）を最大50%削減することに成功している。  

また、BLTは固定語彙を持たないため、任意のバイトグループをパッチとしてマッピングできる。これにより、エンコーダーとデコーダーの軽量な学習モジュールを通じて、柔軟なパッチ表現の生成が可能となった。研究チームは、この手法がトークンベースのモデルよりも効率的なコンピューティングリソースの配分を実現すると指摘している。  

さらに、BLTは従来のトークンベースモデルが抱える効率性とパフォーマンスのトレードオフ問題も解決している。従来モデルでは、処理できる単語や表現の種類（語彙）を増やすと、一度に処理できるデータ量は増えるものの、その分だけモデル全体で必要となる計算処理量も大きく増加してしまうという課題があった。BLTは、データの複雑さに基づいてコンピューティングリソースのバランスを取ることで、この問題を克服している。  

### BLTアプローチ、特に注目すべき点  
トークンに依存しないBLTの性能評価において、特に注目すべき点が3つある。  

１つ目は、推論効率における大幅な改善である。上記でも言及したが、研究チームの実験によると、BLTはLlama 3と同等の性能を維持しながら、推論時のコンピューティングコストを最大50%削減することに成功。データの複雑さに基づいて計算リソースを動的に配分する手法を採用したことが奏功した。  

2つ目は、低頻出言語への対応力の向上である。BLTは、101の異なる言語間の翻訳精度を測定する「FLORES-101」ベンチマークにおける低頻出言語の翻訳タスクで、Llama 3トークナイザーを使用したモデルを上回る性能を示した。英語への翻訳では2ポイント、英語からの翻訳では0.5ポイントの優位性が確認された。特にアルメニア語(1.7%から6.3%へ)、ベンガル語(4.7%から12.7%へ)などの言語で顕著な改善が見られた。  
![image](https://github.com/user-attachments/assets/4521f675-ca61-4b8a-80db-247c2a139fca)  

3つ目は、文字レベルでの理解力の大幅向上である。AIモデルが個々の文字をどれだけ正確に理解し操作できるかを測定できるテスト「CUTE」ベンチマークでは、BLTはトークンベースのLlama 3モデルを25ポイント以上上回る結果を示した。特にスペリング関連タスクでは99.9%という驚異的な正確性を達成。直接バイトレベルで処理を行うBLTの特性が、文字レベルの操作に効果的に機能していることが示された格好である。  
![image](https://github.com/user-attachments/assets/8a86e19b-7fe7-466a-b0bc-cd5aea048466)  

現在のLLM分野は、エージェントシステム開発や推論モデル開発が特に注目を集めているが、トークンベースのアーキテクチャに挑む研究開発はまだ少ないのが現状である。一方、メタのこの研究開発が呼び水となり、BLTを含む多様なアプローチが登場するシナリオも考えられる。  
</details>

# 3/22(Sa)
<details><summary>「GPT-4.5」正式発表 "深い思考"をしなくても世界理解と直観力で性能向上</summary>

米OpenAIは2月28日、生成AIチャットの「ChatGPT」に搭載するAIモデルとして「GPT-4.5」を発表した。同社の「o1」や「o3-mini」などの長く考えて性能を向上する方式は取っておらず、教師なし学習により「GPT-4o」よりも高性能になったという。月額200ドルのProユーザーは同日から利用可能。PlusやTeam、Enterpriseなどの有料プランユーザーには一週間ほどで提供する。  

GPT-4.5では、学習時の計算リソースとデータ拡張、アーキテクチャと最適化の革新により、長く考えず方式でなくても性能を向上させることができたという。その結果、幅広い知識と深い世界理解を備えたモデルとなり、ハルシネーションの提言や幅広いトピックにおける信頼性も向上したとしている。  

GPT-4.5を発表したライブ配信では、o1との比較もライブで実施。o１が返答するのに時間をかけるのに対し、GPT-4.5はすぐに返事を返した。登壇した同社の研究者は回答の内容について「o1も役立つ。多くの情報を出力していて、(質問の)話題を初めて学ぶなら知りたいことがたくさんある」としつつ、「GPT-4.5の答えは流れがずっと自然。アイデアを通じて私の思考をガイドしてくれる」と評した。  
![image](https://github.com/user-attachments/assets/a47f2700-90ab-4807-af79-e20f9cee3242)  

各種ベンチマークテストでは、すべてのスコアでGPT-4oを超えた一方で、o3-miniには一歩及ばないという結果に。これについて同社の研究者は「o3-miniは答える前に考えることができる。GPT-4.5は答える前に考えることができなくても、このような高いスコアを獲得できるのは非常に印象的」と話した。そんな中でもコーディング性能を測るベンチマーク2種のうちの片方（SWE-Lancer Diamond）では、o3-miniの10.8%を超える32.6%の性能を見せている。  

また、OpenAIは今回のGPT-4.5を「研究プレビュー」と位置付けている。OpenAIもこのモデルを実験している段階であり「教師なし学習で出現する能力をユーザーと一緒に探索したい」とした。  
![image](https://github.com/user-attachments/assets/b5f97010-17a9-4e86-8ead-f04eabcfaabc)  
LLM（大規模言語モデル）の事実性を単純ながら難易度の高い知識問題で測定。このテストではo1やo3-miniも抑えてGPT-4.5がトップに  

![image](https://github.com/user-attachments/assets/1e10520c-a2e9-4c7b-ae68-9d0324fb1b66)  
人間のテスターがGPT-4.5とGPT-4oを比較し評価したところ、3種全てでGPT-4.5が上回った  

![image](https://github.com/user-attachments/assets/3ecbbb65-744f-425b-9486-8f3ad7550f1b)  
歴代AIモデルに「なぜ海はしょっぱいのか」と聞いた結果。2018年のGPT-1の回答は「ワードサラダ」だった  

![image](https://github.com/user-attachments/assets/6d7ff6fc-55bd-400a-9550-dee15d88447a)  
2019年のGPT-2になり、間違っているが改善  

![image](https://github.com/user-attachments/assets/05a3788b-07dc-4459-8f96-c701c86967fe)  
2023年のGPT-3.5 Turboで初めて正解に。しかし説明はなく不要な詳細がある  

![image](https://github.com/user-attachments/assets/07b46dcf-084f-4009-85b9-05a0faddd940)
GPT-4 Turboは良い答えに。ただし事実をリストアップしているようでもある  

![image](https://github.com/user-attachments/assets/5bc459ae-4f12-4825-86ba-fae495098d85)  
GPT-4.5は明確で簡潔でまとまりのある答えに  

![image](https://github.com/user-attachments/assets/fbab09e0-0bed-4116-8a1e-4fc4ba8ffde0)  
GPT-4.5とGPT-4o、o3-miniのベンチマーク比較  
</details>
