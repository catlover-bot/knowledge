# 9/8(Mon)

<details><summary>AIがソフトウェアを実際に構築できない理由</summary>

ChatGPTなど多くのLLMが人間並みのコーディング能力を持つようになったというベンチマーク結果が多数登場しており、ソフトウェア開発においてAIが活用される潮流が生じつつある。ところが、コードエディタ「[Zed](https://zed.dev/)」の開発チームの一員であるコンラッド・アーウィン氏は、「LLMはソフトウェアを実際に構築することはできない」として、その理由を説明している。

Why LLMs Can't Really Build Software-Zed's Blog  
https://zed.dev/blog/why-llms-cant-build-software  

https://zed.dev/blog/why-llms-cant-build-software<img width="560" height="350" alt="image" src="https://github.com/user-attachments/assets/b5ca7e4e-cd54-4a10-b187-26ce979ef2f1" />  

アーウィン氏が長年人間のソフトウェアエンジニアの仕事を見ている中で気づいたのは、人間が常に(メンタルモデル)[https://ja.wikipedia.org/wiki/メンタルモデル]を構築するというものであった。メンタルモデルとは、頭の中で行動をイメージするようなもので、有能な人間は大抵の場合以下の動作を繰り返していたとのこと。

- 要件のメンタルモデルを構築する
- その要件を満たすコードを書く
- コードが実際に何をしているかのメンタルモデルを構築する
- 違いを特定し、コードまたは要件を更新する

アーウィン氏は「有能なソフトウェアエンジニアの特筆すべき点は、明確なメンタルモデルを構築し維持する能力である。一方でLLMは違う。LLMはコードを書くのが非常に上手で、問題点を特定して修正する際のコードの更新もある程度は可能である。実際のソフトウェアエンジニアがやるようなこと、コードを読む、テストを実行する、ログを追加するなどもできる。しかし、彼らができないのは、明確なメンタルモデルを維持することである。」と指摘。  
アーウィン氏の肌感覚では「LLMは無限に混乱するもので、自分が書いたコードが実際に機能すると仮定し、テストが失敗すると、コードを修正するべきかテストを修正するべきか推測するしかなく、インライしてすべてを削除して最初からやり直す」という印象があり、ここが人間と大きく違い、与えられた文脈を見直して問題点を導き出すような処理ができないと主張している。  
ソーシャルサイトのHacker Newsでは、「人間は一歩引いて全体を俯瞰しつつ問題の根本原因を特定することができる」と補足されていた。  
アーウィン氏は「人間のソフトウェアエンジニアは作業は進める中でテストを実施する。テストが失敗すると、メンタルモデルを確認してコードを修正するかテストを修正するか、または決定を下す前に追加のデータを集めるかを判断できる。イライラすると、話し合いを通じて助けを求めることができる。そして、時々全てを削除して最初からやり直すこともあるが、その際は問題の理解がより明確になる」と人間の特徴を分析。  

一方、AIには以下のような欠点があるとしている。  

- モデルは欠落した文脈を見つけるのが苦手
- 新しく入力された情報ほど正しいと錯覚しがち
- 誤った情報を事実として主張する幻覚に陥る



アーウィン氏は「要件が明確で、問題が単純であれば、一度で完了できる。ところが、より複雑なタスクでLLMは文脈を正確に維持できず、解決策を提示するための反復作業に取り組めない」とまとめている。

</details>

# 9/9(Tue)

<details><summary>「なぜAIは嘘をつくのか？」OpenAIが論文を公開</summary>

OpenAIは９月５日、言語モデルで発生するハルシネーションのげんいんについて研究結果を公開した。同社では、言語モデルで用いられる事前学習やベンチマークの手法に原因があるとしている。  

ハルシネーションは、言語モデルが生成する回答のうち、もっともらしく見えるが実際には誤っているものを指す。言語モデルの開発における重要な問題で、発生を抑えるための改良が進められている。同社では今回の研究を通じて、ハル氏ネーションが発生する要因として、事前学習の仕組みと、ベンチマークテストの評価手法をあげた。

言語モデルでは、膨大なテキストから次に続く単語を予想するプロセスを通じて事前学習を進めていく。この際、各テキストに正誤のようなラベルはなく、文脈的に正しいかを認識している。このとき、単語のつづりなど一貫したパターンのあるものは学習とともにエラーが発生しなくなるが、論文のタイトルや人の誕生日など、出現頻度の低い任意の情報は予測ができない。これがハルシネーションの原因となる。  

また、標準的なベンチマークテストではAIモデルの性能を正解率の高さで評価することが多い。しかしこの場合、AIモデルが分からないことを「分からない」と答えると評価されないが、推測でそれっぽく答えるとたまたま正解してしまうことがある（誤った場合はハルシネーションになる）。その結果、正直に分からないと答えるより、当てずっぽうで答えるモデルの方がスコアが優位になってしまう現象が起きるという。  

同社では、ハルシネーションの抑制にはAIモデルが「分からない」と答えることが有効だと説明。ハルシネーションの評価手法を新たに導入するだけでは不十分であり、推測での回答を抑制するよう既存のベンチマークを再設計する必要があると指摘した。また、自社のモデルにおいてもハルシネーションの発生率を抑えるために尽力しているとアピール。

</details>

# 9/11(Th)

<details><summary>最低限のNetwork知識</summary>

# ネットワークモデルのoverview
コンピューターのネットワークはコンピュータ同士プロトコルという決まり事に沿って通信を行うことで意思疎通を図っている。このプロトコルは多数あり、類似したものを同じ階層に分けてモデル化し考えるのが一般的である。例えば、以下の図はOSIモデルと呼ばれる７層に分かれたプロトコル。  

- アプリケーション層
プログラマーが意識する部分（HTTP,DNS等）

- プレゼンテーション層
データの表現形式。テキストファイルをASCIIコードのファイルへ変換とか

- セッション層
</details>
